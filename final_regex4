import os
import re
import sys
from collections import Counter
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import lru_cache
from datetime import datetime
from tqdm import tqdm

# ====== CONFIGURATION (fully hardcoded) ====== #
INPUT_FOLDER = r"C:\Users\manav\Documents\pii_logs"   # ðŸ”§ Change this to your input folder
OUTPUT_FOLDER = "output_regex"                        # Fixed name, created in current dir
SUMMARY_FILE = "summary_report.txt"                   # Saved in current working dir
RESUME_LOG = "resume_files.log"                       # âœ… Checkpoint log in current working dir
MAX_WORKERS = 6                                       # Use 6â€“8; 8 by default
# ============================================= #

# ---------- REGEX PATTERNS ----------
PII_PATTERNS = {
    "MOBILE_REGEX": re.compile(r'(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])'),
    "AADHAAR_REGEX": re.compile(r'(?<![A-Za-z0-9])(\d{12})(?![A-Za-z0-9])'),
    "PAN_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{5}\d{4}[A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "GSTIN_REGEX": re.compile(r'(?<![A-Za-z0-9])\d{2}[A-Z]{5}\d{4}[A-Z][1-9A-Z]Z[0-9A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "DL_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{2}\d{2}\d{11}(?![A-Za-z0-9])', re.IGNORECASE),
    "VOTERID_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{3}\d{7}(?![A-Za-z0-9])', re.IGNORECASE),
    "EMAIL_REGEX": re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', re.IGNORECASE),
    "UPI_REGEX": re.compile(r'[A-Za-z0-9.\-_]{2,256}@[A-Za-z]{2,64}'),
    "IP_REGEX": re.compile(r'(?<!\d)(?:\d{1,3}\.){3}\d{1,3}(?!\d)'),
    "MAC_REGEX": re.compile(r'(?<![A-Fa-f0-9])(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}(?![A-Fa-f0-9])'),
    "CARD_REGEX": re.compile(
        r'(?<!\d)('
        r'4\d{12}(?:\d{3})?'                                 # Visa
        r'|5[1-5]\d{14}'                                     # Mastercard (old range)
        r'|2(?:2[2-9]\d{12}|[3-6]\d{13}|7(?:[01]\d{12}|20\d{12}))'  # Mastercard 2-series
        r'|3[47]\d{13}'                                      # Amex
        r'|60\d{14}|65\d{14}|81\d{14}|508\d\d{12}'           # Other BINs
        r')(?!\d)'
    ),
    "COORD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])'
        r'([+-]?(?:90(?:\.0+)?|[0-8]?\d(?:\.\d+)?))'
        r'\s*,\s*'
        r'([+-]?(?:180(?:\.0+)?|1[0-7]\d(?:\.\d+)?|[0-9]?\d(?:\.\d+)?))'
        r'(?![A-Za-z0-9])'
    ),
}

# ---------- SUMMARY (aggregated in parent) ----------
summary = {
    "files_scanned": 0,
    "total_lines": 0,
    "total_matches": 0,
    "total_no_match_lines": 0,
    "per_type_counts": Counter({k: 0 for k in PII_PATTERNS}),
    "blank_files": [],
    "errors": [],
    "output_lines_written": 0
}

# ---------- Aadhaar Verhoeff ----------
@lru_cache(maxsize=10000)
def is_valid_aadhaar(number: str) -> bool:
    if len(number) != 12 or not number.isdigit():
        return False
    mul = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,2,3,4,0,6,7,8,9,5],
        [2,3,4,0,1,7,8,9,5,6],
        [3,4,0,1,2,8,9,5,6,7],
        [4,0,1,2,3,9,5,6,7,8],
        [5,9,8,7,6,0,4,3,2,1],
        [6,5,9,8,7,1,0,4,3,2],
        [7,6,5,9,8,2,1,0,4,3],
        [8,7,6,5,9,3,2,1,0,4],
        [9,8,7,6,5,4,3,2,1,0]
    ]
    perm = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,5,7,6,2,8,3,0,9,4],
        [5,8,0,3,7,9,6,1,4,2],
        [8,9,1,6,0,4,3,5,2,7],
        [9,4,5,3,1,2,6,8,7,0],
        [4,2,8,6,5,7,3,9,0,1],
        [2,7,9,3,8,0,6,4,1,5],
        [7,0,4,6,9,1,3,2,5,8]
    ]
    inv = [0,4,3,2,1,5,6,7,8,9]
    c = 0
    for i, ch in enumerate(reversed(number)):
        c = mul[c][perm[i % 8][int(ch)]]
    return inv[c] == 0

# ---------- Core matching ----------
def extract_matches(log_line: str, per_type_counts: Counter):
    matches = []
    for pii_type, pattern in PII_PATTERNS.items():
        if not pattern.search(log_line):
            continue
        for m in pattern.finditer(log_line):
            value = m.group(0)
            if pii_type == "AADHAAR_REGEX" and not is_valid_aadhaar(value):
                continue
            matches.append((value, pii_type))
            per_type_counts[pii_type] += 1
    return matches

def process_file(file_path: str) -> dict:
    """
    Runs in a separate process. Writes to <output>.part then atomically renames.
    Returns a compact dict with stats for aggregation.
    """
    local = {
        "file_name": os.path.basename(file_path),
        "lines": 0,
        "no_match_lines": 0,
        "written": 0,
        "per_type_counts": Counter({k: 0 for k in PII_PATTERNS}),
        "error": None,
    }
    try:
        final_out = os.path.join(OUTPUT_FOLDER, os.path.basename(file_path))
        tmp_out = final_out + ".part"

        # Always recreate .part fresh (safe to reprocess a single file if last run died mid-file)
        with open(file_path, "r", encoding="utf-8", errors="ignore", buffering=1024*1024) as f_in, \
             open(tmp_out, "w", encoding="utf-8", buffering=1024*1024) as f_out:

            for raw in f_in:
                local["lines"] += 1
                line = raw.rstrip("\n")

                if ";" in line:
                    log_line, path = line.rsplit(";", 1)
                    log_line = log_line.rstrip()
                    path = path.strip()
                else:
                    log_line, path = line, "UNKNOWN_PATH"

                matches = extract_matches(log_line, local["per_type_counts"])
                if matches:
                    for value, pii_type in matches:
                        f_out.write(f"{log_line} ; {path} ;{value};{pii_type}\n")
                        local["written"] += 1
                else:
                    local["no_match_lines"] += 1

        # Atomic replace -> ensures only completed files appear without ".part"
        os.replace(tmp_out, final_out)

    except Exception as e:
        # Best-effort cleanup of partial file if any error
        try:
            if 'tmp_out' in locals() and os.path.exists(tmp_out):
                os.remove(tmp_out)
        except Exception:
            pass
        local["error"] = f"{local['file_name']}: {e}"
    return local

# ---------- Resume helpers (parent process only) ----------
def load_completed_set(log_path: str) -> set:
    completed = set()
    if os.path.exists(log_path):
        with open(log_path, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                name = line.strip()
                if name and not name.startswith("#"):
                    completed.add(name)
    return completed

def append_completed(log_path: str, file_name: str):
    # Append after each successful file to persist progress immediately
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(file_name + "\n")
        f.flush()  # ensure itâ€™s on disk

def write_summary(input_folder: str):
    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write(f"Summary Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Input Folder: {input_folder}\n")
        f.write(f"Output Folder: {os.path.abspath(OUTPUT_FOLDER)}\n\n")
        f.write(f"Total Files Scanned (this run): {summary['files_scanned']}\n")
        f.write(f"Total Lines Scanned (this run): {summary['total_lines']}\n")
        f.write(f"Total PII Matches Found (this run): {summary['total_matches']}\n")
        f.write(f"Total Output Lines Written (this run): {summary['output_lines_written']}\n")
        f.write(f"Total Lines with NO_MATCH (this run): {summary['total_no_match_lines']}\n\n")

        f.write("PII Type Counts (this run):\n")
        for pii_type, count in summary['per_type_counts'].items():
            f.write(f"- {pii_type}: {count}\n")

        if summary['blank_files']:
            f.write("\nFiles with No Matches or Blank (this run):\n")
            for fname in summary['blank_files']:
                f.write(f"- {fname}\n")

        if summary['errors']:
            f.write("\nErrors (this run):\n")
            for err in summary['errors']:
                f.write(f"- {err}\n")

def main():
    if not os.path.isdir(INPUT_FOLDER):
        print(f"ERROR: INPUT_FOLDER does not exist: {INPUT_FOLDER}", file=sys.stderr)
        sys.exit(1)

    os.makedirs(OUTPUT_FOLDER, exist_ok=True)

    # Gather candidate .txt input files
    all_files = [
        os.path.join(INPUT_FOLDER, f)
        for f in os.listdir(INPUT_FOLDER)
        if f.endswith(".txt") and os.path.isfile(os.path.join(INPUT_FOLDER, f))
    ]

    if not all_files:
        print("No .txt files found in INPUT_FOLDER.", file=sys.stderr)
        sys.exit(2)

    # Load resume log & build skip set
    completed = load_completed_set(RESUME_LOG)

    # Also treat already-finalized outputs as completed (defensive if log was deleted)
    for f in list(all_files):
        out_final = os.path.join(OUTPUT_FOLDER, os.path.basename(f))
        if os.path.exists(out_final):
            completed.add(os.path.basename(f))

    # Pending files = not completed yet
    pending_files = [fp for fp in all_files if os.path.basename(fp) not in completed]

    if not pending_files:
        print("All files already processed per resume log / outputs. Nothing to do.")
        # Still write an empty summary for this run
        write_summary(INPUT_FOLDER)
        return

    # Kick off pool; as_completed ensures immediate pickup of next task by free workers
    overall_bar = tqdm(total=len(pending_files), desc="Overall", unit="file", leave=True)

    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = {ex.submit(process_file, fp): fp for fp in pending_files}

        for fut in as_completed(futures):
            src = futures[fut]
            local = fut.result()

            summary["files_scanned"] += 1
            summary["total_lines"] += local["lines"]
            summary["total_no_match_lines"] += local["no_match_lines"]
            summary["output_lines_written"] += local["written"]
            summary["per_type_counts"].update(local["per_type_counts"])

            # Mark "blank" when no output lines were written (even if file had lines)
            if local["lines"] == 0 or local["written"] == 0:
                summary["blank_files"].append(local["file_name"])

            if local["error"]:
                summary["errors"].append(local["error"])
            else:
                # âœ… Append to resume log only on success
                append_completed(RESUME_LOG, os.path.basename(src))

            overall_bar.update(1)

    overall_bar.close()
    summary["total_matches"] = sum(summary["per_type_counts"].values())
    write_summary(INPUT_FOLDER)

if __name__ == "__main__":
    # Quick tweak: set MAX_WORKERS = 6 above if you prefer 6.
    # Delete resume_files.log to force a full re-run from scratch.
    main()

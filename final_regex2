import os
import re
import sys
import json
import shutil
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import List, Tuple
from tqdm import tqdm
from datetime import datetime

# ========== SET INPUT AND OUTPUT FOLDERS HERE ==========
input_dir = Path("new_logs")  # ðŸ”§ Replace with your input folder name
output_dir = Path("output")
report_path = Path("summary_report.txt")
log_file_path = Path("log.txt")
# =======================================================

# Aadhaar Verhoeff checksum tables
verhoeff_d = [
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
    [1, 2, 3, 4, 0, 6, 7, 8, 9, 5],
    [2, 3, 4, 0, 1, 7, 8, 9, 5, 6],
    [3, 4, 0, 1, 2, 8, 9, 5, 6, 7],
    [4, 0, 1, 2, 3, 9, 5, 6, 7, 8],
    [5, 9, 8, 7, 6, 0, 4, 3, 2, 1],
    [6, 5, 9, 8, 7, 1, 0, 4, 3, 2],
    [7, 6, 5, 9, 8, 2, 1, 0, 4, 3],
    [8, 7, 6, 5, 9, 3, 2, 1, 0, 4],
    [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
]
verhoeff_p = [
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
    [1, 5, 7, 6, 2, 8, 3, 0, 9, 4],
    [5, 8, 0, 3, 7, 9, 6, 1, 4, 2],
    [8, 9, 1, 6, 0, 4, 3, 5, 2, 7],
    [9, 4, 5, 3, 1, 2, 6, 8, 7, 0],
    [4, 2, 8, 6, 5, 7, 3, 9, 0, 1],
    [2, 7, 9, 3, 8, 0, 6, 4, 1, 5],
    [7, 0, 4, 6, 9, 1, 3, 2, 5, 8]
]

def is_valid_aadhaar(num: str) -> bool:
    c = 0
    try:
        for i, item in enumerate(reversed(num)):
            c = verhoeff_d[c][verhoeff_p[i % 8][int(item)]]
        return c == 0
    except:
        return False

# Updated PII Patterns
PII_PATTERNS = {
    "AADHAAR_REGEX": re.compile(r'(?<!\d)(\d{12})(?!\d)'),
    "DL_REGEX": re.compile(r'(?<!\w)[A-Z]{2}[0-9]{2}[-\s]?[0-9]{4}[0-9]{7}(?!\w)', re.IGNORECASE),
    "GSTIN_REGEX": re.compile(r'(?<!\w)[0-9]{2}[A-Z]{5}[0-9]{4}[A-Z][A-Z0-9]Z[A-Z0-9](?!\w)', re.IGNORECASE),
    "IP_REGEX": re.compile(r'(?<!\d)(?:[0-9]{1,3}\.){3}[0-9]{1,3}(?!\d)'),
    "MAC_REGEX": re.compile(r'(?<![A-Fa-f0-9])(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}(?![A-Fa-f0-9])'),
    "EMAIL_REGEX": re.compile(r'[\w\.-]+@[\w\.-]+\.\w{2,3}'),
    "MOBILE_REGEX": re.compile(r'(?<!\d)(?:\+91|91|0)?[6-9][0-9]{9}(?!\d)'),
    "PAN_REGEX": re.compile(r'(?<!\w)[A-Z]{5}[0-9]{4}[A-Z](?!\w)', re.IGNORECASE),
    "UPI_REGEX": re.compile(r'[a-zA-Z0-9.\-_]{2,}@[a-zA-Z]{2,}'),
    "VOTERID_REGEX": re.compile(r'(?<!\w)[A-Z]{3}[0-9]{7}(?!\w)', re.IGNORECASE),
    "CARD_REGEX": re.compile(
        r'(?<!\d)('
        r'4[0-9]{12}(?:[0-9]{3})?'
        r'|5[1-5][0-9]{14}'
        r'|2(?:2[2-9][0-9]{12}|[3-6][0-9]{13}|7(?:[01][0-9]{12}|20[0-9]{12}))'
        r'|3[47][0-9]{13}'
        r'|60[0-9]{14}'
        r'|65[0-9]{14}'
        r'|81[0-9]{14}'
        r'|508[0-9][0-9]{12}'
        r')(?!\d)'
    )
}

def extract_matches(line: str) -> List[Tuple[str, str]]:
    matches = []
    for pii_type, pattern in PII_PATTERNS.items():
        for match in pattern.finditer(line):
            val = match.group()
            if pii_type == "AADHAAR_REGEX" and not is_valid_aadhaar(val):
                continue
            matches.append((val, pii_type))
    return matches

def process_file(file_path: Path) -> Tuple[str, dict]:
    rel_path = file_path.relative_to(input_dir)
    output_file = output_dir / rel_path
    output_file.parent.mkdir(parents=True, exist_ok=True)

    stats = {
        "lines_read": 0,
        "lines_written": 0,
        "lines_skipped_no_match": 0,
        "lines_added_due_to_duplicates": 0,
        "matches_per_type": {k: 0 for k in PII_PATTERNS},
        "blank_file": True,
        "errors": []
    }

    lines_out = []
    try:
        with file_path.open("r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                stats["lines_read"] += 1
                if ";" not in line:
                    stats["errors"].append(f"{rel_path}: missing semicolon")
                    continue
                log, path = line.rsplit(";", 1)
                matches = extract_matches(log)
                if not matches:
                    stats["lines_skipped_no_match"] += 1
                    lines_out.append(line + f";{path};NO_MATCH")
                    continue
                for idx, (val, pii_type) in enumerate(matches):
                    new_line = f"{log};{path};{val};{pii_type}"
                    lines_out.append(new_line)
                    stats["matches_per_type"][pii_type] += 1
                    if idx > 0:
                        stats["lines_added_due_to_duplicates"] += 1
                stats["lines_written"] += 1
                stats["blank_file"] = False
    except Exception as e:
        stats["errors"].append(f"{rel_path}: {str(e)}")

    # Write output even if blank
    with output_file.open("w", encoding="utf-8") as out:
        for line in lines_out:
            out.write(line + "\n")

    return str(rel_path), stats

def run_pii_extraction():
    if output_dir.exists():
        shutil.rmtree(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    all_files = list(input_dir.rglob("*.txt"))
    summary = {
        "total_files": 0,
        "total_lines_read": 0,
        "total_lines_written": 0,
        "total_lines_added_due_to_duplicates": 0,
        "total_lines_skipped_no_match": 0,
        "matches_per_type": {k: 0 for k in PII_PATTERNS},
        "blank_files": [],
        "errors": []
    }

    with report_path.open("w", encoding="utf-8") as rpt, log_file_path.open("w", encoding="utf-8") as logf:
        rpt.write(f"PII Regex Scan Report - {datetime.now()}\n\n")
        logf.write(f"Log File - {datetime.now()}\n\n")

        with ProcessPoolExecutor(max_workers=4) as executor:
            futures = {executor.submit(process_file, f): f for f in all_files}
            for future in tqdm(as_completed(futures), total=len(futures), desc="Processing"):
                rel_path, stats = future.result()
                summary["total_files"] += 1
                summary["total_lines_read"] += stats["lines_read"]
                summary["total_lines_written"] += stats["lines_written"]
                summary["total_lines_skipped_no_match"] += stats["lines_skipped_no_match"]
                summary["total_lines_added_due_to_duplicates"] += stats["lines_added_due_to_duplicates"]
                for k in stats["matches_per_type"]:
                    summary["matches_per_type"][k] += stats["matches_per_type"][k]
                if stats["blank_file"]:
                    summary["blank_files"].append(rel_path)
                for err in stats["errors"]:
                    summary["errors"].append(err)
                    logf.write(err + "\n")

        # Final summary write
        rpt.write(f"Total files scanned: {summary['total_files']}\n")
        rpt.write(f"Total lines read: {summary['total_lines_read']}\n")
        rpt.write(f"Total lines written to output: {summary['total_lines_written']}\n")
        rpt.write(f"Lines skipped (no match): {summary['total_lines_skipped_no_match']}\n")
        rpt.write(f"Lines added due to multiple matches: {summary['total_lines_added_due_to_duplicates']}\n")
        rpt.write(f"Final output lines (approx): {summary['total_lines_written'] + summary['total_lines_added_due_to_duplicates']}\n\n")
        rpt.write("Match counts by type:\n")
        for k, v in summary["matches_per_type"].items():
            rpt.write(f"  {k}: {v}\n")
        if summary["blank_files"]:
            rpt.write("\nBlank files:\n")
            for f in summary["blank_files"]:
                rpt.write(f"  {f}\n")
        if summary["errors"]:
            rpt.write("\nErrors:\n")
            for e in summary["errors"]:
                rpt.write(f"  {e}\n")

if __name__ == "__main__":
    run_pii_extraction()

import os
import re
import sys
import time
import traceback
from collections import Counter, defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import lru_cache
from datetime import datetime
from tqdm import tqdm

# ====== CONFIGURATION (fully hardcoded) ====== #
INPUT_FOLDER = r"C:\Users\manav\Documents\pii_logs"   # ðŸ”§ Change this to your input folder
OUTPUT_FOLDER = "output_regex"                        # Fixed name, created in current dir
SUMMARY_FILE = "summary_report.txt"                   # Saved in current working dir
RESUME_LOG = "resume_files.log"                       # âœ… Checkpoint log in current working dir
MAX_WORKERS = 6                                       # Use 6â€“8; 8 by default
SUMMARY_EVERY_SECS = 30                               # â±ï¸ periodic summary flush
# Long-line safety rails (EMAIL/UPI only)
LONG_LINE_WINDOW = 50_000         # > this: window EMAIL/UPI around '@'
HARD_SKIP_LEN    = 1_000_000      # >= this: skip EMAIL/UPI entirely for that line
EMAIL_WIN_L, EMAIL_WIN_R = 256, 320
UPI_WIN_L,   UPI_WIN_R   = 256,  64
MAX_ATS_PER_LINE = 2000
# ============================================= #

# ---------- REGEX PATTERNS ----------
PII_PATTERNS = {
    "MOBILE_REGEX": re.compile(r'(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])'),
    "AADHAAR_REGEX": re.compile(r'(?<![A-Za-z0-9])(\d{12})(?![A-Za-z0-9])'),
    "PAN_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{5}\d{4}[A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "GSTIN_REGEX": re.compile(r'(?<![A-Za-z0-9])\d{2}[A-Z]{5}\d{4}[A-Z][1-9A-Z]Z[0-9A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "DL_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{2}\d{2}\d{11}(?![A-Za-z0-9])', re.IGNORECASE),
    "VOTERID_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{3}\d{7}(?![A-Za-z0-9])', re.IGNORECASE),
    # EMAIL: TLD >=2 chars (e.g., .co, .com, .co.in etc.). Case-insensitive.
    "EMAIL_REGEX": re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', re.IGNORECASE),
    "UPI_REGEX": re.compile(r'[A-Za-z0-9.\-_]{2,256}@[A-Za-z]{2,64}'),
    # IPv4 strict 0â€“255
    "IP_REGEX": re.compile(
        r'(?<!\d)'
        r'(?:(?:25[0-5]|2[0-4]\d|1?\d?\d)\.){3}'
        r'(?:25[0-5]|2[0-4]\d|1?\d?\d)'
        r'(?!\d)'
    ),
    "MAC_REGEX": re.compile(r'(?<![A-Fa-f0-9])(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}(?![A-Fa-f0-9])'),
    # Card numbers with alnum lookarounds; we'll further validate with Luhn.
    "CARD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])('
        r'4\d{12}(?:\d{3})?'                                 # Visa
        r'|5[1-5]\d{14}'                                     # Mastercard (old range)
        r'|2(?:2[2-9]\d{12}|[3-6]\d{13}|7(?:[01]\d{12}|20\d{12}))'  # Mastercard 2-series
        r'|3[47]\d{13}'                                      # Amex
        r'|60\d{14}|65\d{14}|81\d{14}|508\d\d{12}'           # Other BINs
        r')(?![A-Za-z0-9])'
    ),
    # Coords: decimals required for both lat/lon
    "COORD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])'
        r'([+-]?(?:90\.(?:0+)?|[0-8]?\d\.\d+))'
        r'\s*,\s*'
        r'([+-]?(?:180\.(?:0+)?|1[0-7]\d\.\d+|[0-9]?\d\.\d+))'
        r'(?![A-Za-z0-9])'
    ),
}

# ---------- SUMMARY (aggregated in parent) ----------
summary = {
    # discovery & resume
    "files_found": 0,
    "skipped_files": [],

    # per-run processing
    "files_scanned": 0,
    "files_success": 0,
    "files_error": 0,
    "blank_input_files": [],   # input had 0 lines
    "zero_match_files": [],    # had lines but wrote none
    "errors": [],              # "file: reason"

    # counters
    "total_lines": 0,                  # input lines scanned
    "output_lines_written": 0,         # lines written (one per (value,type) match)
    "total_no_match_lines": 0,         # recomputed at write
    "per_type_counts": Counter({k: 0 for k in PII_PATTERNS}),
    "total_matches": 0,                # recomputed at write: sum(per_type_counts)

    # long-line telemetry
    "email_windowed_lines": 0,
    "email_too_long_lines": 0,
    "upi_windowed_lines": 0,
    "upi_too_long_lines": 0,

    # timing/config
    "start_ts": None,
    "end_ts": None,
    "max_workers": MAX_WORKERS,
}

# ---------- Aadhaar Verhoeff ----------
@lru_cache(maxsize=10000)
def is_valid_aadhaar(number: str) -> bool:
    if len(number) != 12 or not number.isdigit():
        return False
    mul = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,2,3,4,0,6,7,8,9,5],
        [2,3,4,0,1,7,8,9,5,6],
        [3,4,0,1,2,8,9,5,6,7],
        [4,0,1,2,3,9,5,6,7,8],
        [5,9,8,7,6,0,4,3,2,1],
        [6,5,9,8,7,1,0,4,3,2],
        [7,6,5,9,8,2,1,0,4,3],
        [8,7,6,5,9,3,2,1,0,4],
        [9,8,7,6,5,4,3,2,1,0]
    ]
    perm = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,5,7,6,2,8,3,0,9,4],
        [5,8,0,3,7,9,6,1,4,2],
        [8,9,1,6,0,4,3,5,2,7],
        [9,4,5,3,1,2,6,8,7,0],
        [4,2,8,6,5,7,3,9,0,1],
        [2,7,9,3,8,0,6,4,1,5],
        [7,0,4,6,9,1,3,2,5,8]
    ]
    inv = [0,4,3,2,1,5,6,7,8,9]
    c = 0
    for i, ch in enumerate(reversed(number)):
        c = mul[c][perm[i % 8][int(ch)]]
    return inv[c] == 0

# ---------- Card Luhn ----------
def _luhn_valid(num: str) -> bool:
    s, alt = 0, False
    for ch in reversed(num):
        if not ch.isdigit():
            return False
        n = ord(ch) - 48
        if alt:
            n *= 2
            if n > 9:
                n -= 9
        s += n
        alt = not alt
    return (s % 10) == 0

# ---------- EMAIL/UPI windowing helpers ----------
def _windows_around_at(line: str, left: int, right: int, max_ats: int):
    idxs = [i for i, ch in enumerate(line) if ch == '@']
    if len(idxs) > max_ats:
        idxs = idxs[:max_ats]
    L = len(line)
    for i in idxs:
        start = max(0, i - left)
        end   = min(L, i + 1 + right)
        yield line[start:end]

# ---------- Core matching (dedup per line) ----------
def extract_matches(log_line: str) -> tuple[set[tuple[str,str]], str | None, str | None]:
    """
    Returns:
      - unique_matches: set of (value, pii_type)
      - email_scan_mode: 'FULL' | 'WINDOWED' | 'TOO_LONG' | None
      - upi_scan_mode:   'FULL' | 'WINDOWED' | 'TOO_LONG' | None
    Applies Aadhaar Verhoeff and Luhn (cards). Dedups repeats within the same line.
    """
    unique = set()
    email_mode = None
    upi_mode = None

    # EMAIL
    if "EMAIL_REGEX" in PII_PATTERNS:
        if len(log_line) >= HARD_SKIP_LEN:
            email_mode = "TOO_LONG"
        elif len(log_line) > LONG_LINE_WINDOW:
            email_mode = "WINDOWED"
            for chunk in _windows_around_at(log_line, EMAIL_WIN_L, EMAIL_WIN_R, MAX_ATS_PER_LINE):
                for m in PII_PATTERNS["EMAIL_REGEX"].finditer(chunk):
                    unique.add((m.group(0), "EMAIL_REGEX"))
        else:
            email_mode = "FULL"
            for m in PII_PATTERNS["EMAIL_REGEX"].finditer(log_line):
                unique.add((m.group(0), "EMAIL_REGEX"))

    # UPI
    if "UPI_REGEX" in PII_PATTERNS:
        if len(log_line) >= HARD_SKIP_LEN:
            upi_mode = "TOO_LONG"
        elif len(log_line) > LONG_LINE_WINDOW:
            upi_mode = "WINDOWED"
            for chunk in _windows_around_at(log_line, UPI_WIN_L, UPI_WIN_R, MAX_ATS_PER_LINE):
                for m in PII_PATTERNS["UPI_REGEX"].finditer(chunk):
                    unique.add((m.group(0), "UPI_REGEX"))
        else:
            upi_mode = "FULL"
            for m in PII_PATTERNS["UPI_REGEX"].finditer(log_line):
                unique.add((m.group(0), "UPI_REGEX"))

    # OTHER PATTERNS (full-line)
    for pii_type, pattern in PII_PATTERNS.items():
        if pii_type in ("EMAIL_REGEX", "UPI_REGEX"):
            continue
        if not pattern.search(log_line):
            continue
        for m in pattern.finditer(log_line):
            value = m.group(0)
            if pii_type == "AADHAAR_REGEX" and not is_valid_aadhaar(value):
                continue
            if pii_type == "CARD_REGEX" and not _luhn_valid(value):
                continue
            unique.add((value, pii_type))

    return unique, email_mode, upi_mode

def process_file(file_path: str) -> dict:
    """
    Runs in a separate process. Writes directly to final output (overwrite).
    On error, partial output is removed so the file is retried next run.
    Output format: log_line ; path ; value ; pii_type
    """
    local = {
        "file_name": os.path.basename(file_path),
        "lines": 0,
        "no_match_lines": 0,
        "written": 0,
        "per_type_counts": Counter({k: 0 for k in PII_PATTERNS}),
        "error": None,
        "input_was_blank": False,
        # long-line telemetry
        "email_windowed_lines": 0,
        "email_too_long_lines": 0,
        "upi_windowed_lines": 0,
        "upi_too_long_lines": 0,
    }
    out_path = os.path.join(OUTPUT_FOLDER, os.path.basename(file_path))

    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore", buffering=1024*1024) as f_in, \
             open(out_path,  "w", encoding="utf-8", buffering=1024*1024) as f_out:

            any_line = False
            for raw in f_in:
                any_line = True
                local["lines"] += 1
                line = raw.rstrip("\n")

                if ";" in line:
                    log_line, path = line.rsplit(";", 1)
                    log_line = log_line.rstrip()
                    path = path.strip()
                else:
                    log_line, path = line, "UNKNOWN_PATH"

                unique_matches, email_mode, upi_mode = extract_matches(log_line)

                # record email/upi scan mode per line
                if email_mode == "WINDOWED": local["email_windowed_lines"] += 1
                elif email_mode == "TOO_LONG": local["email_too_long_lines"] += 1
                if upi_mode == "WINDOWED": local["upi_windowed_lines"] += 1
                elif upi_mode == "TOO_LONG": local["upi_too_long_lines"] += 1

                if unique_matches:
                    # write each unique (value,type) pair once per line
                    for value, pii_type in sorted(unique_matches):
                        f_out.write(f"{log_line} ; {path} ;{value};{pii_type}\n")
                        local["written"] += 1
                        local["per_type_counts"][pii_type] += 1
                else:
                    local["no_match_lines"] += 1

            if not any_line:
                local["input_was_blank"] = True
                # empty output file already created

    except Exception as e:
        try:
            if os.path.exists(out_path):
                os.remove(out_path)
        except Exception:
            pass
        err = f"{local['file_name']}: {e.__class__.__name__}: {e}"
        if isinstance(e, OSError) and getattr(e, 'errno', None) is not None:
            err += f" (errno={e.errno})"
        err += "\n" + "".join(traceback.format_exception_only(type(e), e)).strip()
        local["error"] = err

    return local

# ---------- Resume helpers (parent process only) ----------
def load_completed_set(log_path: str) -> set:
    completed = set()
    if os.path.exists(log_path):
        with open(log_path, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                name = line.strip()
                if name and not line.startswith("#"):
                    completed.add(name)
    return completed

def append_completed(log_path: str, file_name: str):
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(file_name + "\n")
        f.flush()

# ---------- Summary writer ----------
def write_summary(input_folder: str, all_files=None, pending_files=None, completed_set=None):
    summary["end_ts"] = time.time()
    summary["total_matches"] = sum(summary["per_type_counts"].values())
    summary["total_no_match_lines"] = summary["total_lines"] - summary["output_lines_written"]

    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write(f"Summary Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Input Folder: {input_folder}\n")
        f.write(f"Output Folder: {os.path.abspath(OUTPUT_FOLDER)}\n")
        f.write(f"Max Workers: {summary['max_workers']}\n\n")

        if all_files is not None and pending_files is not None and completed_set is not None:
            skipped = sorted([os.path.basename(fp) for fp in all_files if os.path.basename(fp) in completed_set])
            summary["skipped_files"] = skipped
            f.write("=== File Discovery ===\n")
            f.write(f"Total .txt files found: {len(all_files)}\n")
            f.write(f"Already completed (skipped this run): {len(skipped)}\n")
            f.write(f"Pending at start of run: {len(pending_files)}\n\n")

        f.write("=== Processing (this run) ===\n")
        f.write(f"Files processed: {summary['files_scanned']}\n")
        f.write(f" - Success: {summary['files_success']}\n")
        f.write(f" - Errors:  {summary['files_error']}\n")
        f.write(f" - Blank input files: {len(summary['blank_input_files'])}\n")
        f.write(f" - Files with 0 matches (had lines): {len(summary['zero_match_files'])}\n\n")

        f.write("=== Line Counts (this run) ===\n")
        f.write(f"Total Lines Scanned: {summary['total_lines']}\n")
        f.write(f"Lines Written (unique matches): {summary['output_lines_written']}\n")
        f.write(f"Lines Removed (no match): {summary['total_no_match_lines']}\n\n")

        f.write("=== PII Type Counts (this run) ===\n")
        for pii_type, count in sorted(summary['per_type_counts'].items()):
            f.write(f"- {pii_type}: {count}\n")
        f.write("\n")

        f.write("=== Long-line telemetry (EMAIL/UPI) ===\n")
        f.write(f"Email: windowed lines = {summary['email_windowed_lines']}, too-long-skipped = {summary['email_too_long_lines']}\n")
        f.write(f"UPI:   windowed lines = {summary['upi_windowed_lines']}, too-long-skipped = {summary['upi_too_long_lines']}\n")

        if summary['skipped_files']:
            f.write("\n=== Skipped Files (already completed) ===\n")
            for fname in summary['skipped_files']:
                f.write(f"- {fname}\n")

        if summary['blank_input_files']:
            f.write("\n=== Blank Input Files (0 lines) ===\n")
            for fname in sorted(summary['blank_input_files']):
                f.write(f"- {fname}\n")

        if summary['zero_match_files']:
            f.write("\n=== Files With 0 Matches (had lines) ===\n")
            for fname in sorted(summary['zero_match_files']):
                f.write(f"- {fname}\n")

        if summary['errors']:
            f.write("\n=== Errors (this run) ===\n")
            for err in summary['errors']:
                f.write(f"- {err}\n")

        f.write(f"\nSanity: sum(per-type)={summary['total_matches']}  vs output_lines={summary['output_lines_written']}\n")

# ---------- Main ----------
def main():
    if not os.path.isdir(INPUT_FOLDER):
        print(f"ERROR: INPUT_FOLDER does not exist: {INPUT_FOLDER}", file=sys.stderr)
        sys.exit(1)

    summary["start_ts"] = time.time()
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)

    # Discover inputs (stable order)
    all_files = sorted(
        os.path.join(INPUT_FOLDER, f)
        for f in os.listdir(INPUT_FOLDER)
        if f.endswith(".txt") and os.path.isfile(os.path.join(INPUT_FOLDER, f))
    )
    summary["files_found"] = len(all_files)

    if not all_files:
        print("No .txt files found in INPUT_FOLDER.", file=sys.stderr)
        write_summary(INPUT_FOLDER, [], [], set())
        sys.exit(2)

    # Resume set: ONLY resume log (do NOT infer from existing outputs)
    completed = load_completed_set(RESUME_LOG)
    pending_files = [fp for fp in all_files if os.path.basename(fp) not in completed]

    if not pending_files:
        print("All files already processed per resume log. Nothing to do.")
        write_summary(INPUT_FOLDER, all_files, pending_files, completed)
        return

    # Prime summary
    write_summary(INPUT_FOLDER, all_files, pending_files, completed)

    overall_bar = tqdm(total=len(pending_files), desc="Overall", unit="file", leave=True)
    last_summary_write = time.time()

    try:
        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:
            futures = {ex.submit(process_file, fp): fp for fp in pending_files}

            for fut in as_completed(futures):
                src = futures[fut]
                base = os.path.basename(src)

                try:
                    local = fut.result()
                except Exception as e:
                    summary["files_scanned"] += 1
                    summary["files_error"] += 1
                    summary["errors"].append(f"{base}: worker exception: {e}")
                    overall_bar.update(1)
                    if time.time() - last_summary_write >= SUMMARY_EVERY_SECS:
                        write_summary(INPUT_FOLDER, all_files, pending_files, completed)
                        last_summary_write = time.time()
                    continue

                summary["files_scanned"] += 1
                summary["total_lines"] += local["lines"]
                summary["output_lines_written"] += local["written"]
                summary["per_type_counts"].update(local["per_type_counts"])

                # long-line telemetry
                summary["email_windowed_lines"] += local["email_windowed_lines"]
                summary["email_too_long_lines"] += local["email_too_long_lines"]
                summary["upi_windowed_lines"]   += local["upi_windowed_lines"]
                summary["upi_too_long_lines"]   += local["upi_too_long_lines"]

                if local["input_was_blank"]:
                    summary["blank_input_files"].append(local["file_name"])
                if local["lines"] > 0 and local["written"] == 0:
                    summary["zero_match_files"].append(local["file_name"])

                if local["error"]:
                    summary["files_error"] += 1
                    summary["errors"].append(local["error"])
                else:
                    summary["files_success"] += 1
                    append_completed(RESUME_LOG, base)

                overall_bar.update(1)

                if time.time() - last_summary_write >= SUMMARY_EVERY_SECS:
                    write_summary(INPUT_FOLDER, all_files, pending_files, completed)
                    last_summary_write = time.time()

    finally:
        overall_bar.close()
        write_summary(INPUT_FOLDER, all_files, pending_files, completed)

if __name__ == "__main__":
    main()

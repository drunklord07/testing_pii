#!/usr/bin/env python3
# === EDIT THESE TWO LINES ===
INPUT_ROOT = r"C:\Users\dhhgdh\Desktop\path_pro"   # root that holds your 13 subfolders (do NOT set this to the parent)
WORKERS    = 4                                      # number of worker threads

# =====================================================================
# No changes needed below this line.
# =====================================================================

import concurrent.futures
import hashlib
import logging
import os
import re
import sys
import threading
from collections import defaultdict
from typing import Tuple, Optional, Dict, List

# Output folder and log live in the folder you RUN the script from (CWD)
OUTPUT_DIR_NAME = "final_output"        # Flat output folder in CWD
LOG_FILENAME    = "processing.txt"      # Log file (TXT as requested)

# Detect timestamped lines at the start: "YYYY-MM-DD HH:MM:SS,mmm"
TS_RE = re.compile(r"^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2},\d{3}")

# Skip these directories (plus any that start with '.')
SKIP_DIR_NAMES = {
    OUTPUT_DIR_NAME,
    "$RECYCLE.BIN",
    "System Volume Information",
    "__pycache__",
    ".git",
}

class Counters:
    """Thread-safe global counters."""
    def __init__(self):
        self.lock = threading.Lock()
        self.total_txt_files_found = 0
        self.total_files_selected = 0
        self.total_files_processed = 0
        self.total_files_skipped = 0
        self.total_files_skipped_duplicates = 0
        self.total_dirs_skipped = 0
        self.total_lines_scanned = 0
        self.total_logs_modified = 0
        self.errors = 0

    def add(self, **kwargs):
        with self.lock:
            for k, v in kwargs.items():
                setattr(self, k, getattr(self, k) + v)

def setup_logging(log_path: str):
    """Configure logging to file + console (INFO)."""
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_path, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )
    # Keep console less noisy; file captures DEBUG
    logging.getLogger().handlers[-1].setLevel(logging.INFO)

def _normalize_slashes(p: str) -> str:
    return "/".join(p.split(os.sep))

def posix_rel_dir_excluding_root(root_dir: str, file_path: str) -> str:
    """
    Return POSIX-style relative directory path BELOW the given root (never includes root name).
    Example:
        root=.../path_pro, file=.../path_pro/middle/pii/app.txt -> "middle/pii"
    Extra guard: if for any reason relpath includes the root name as first segment, strip it.
    """
    root_abs   = os.path.abspath(root_dir)
    root_name  = os.path.basename(root_abs.rstrip("\\/"))
    file_dir   = os.path.dirname(os.path.abspath(file_path))
    rel        = os.path.relpath(file_dir, root_abs)
    rel_norm   = _normalize_slashes(rel)
    if rel_norm == ".":
        return ""  # directly under root
    parts = [p for p in rel_norm.split("/") if p not in ("", ".")]
    # Guard against accidental inclusion of root segment due to casing or bad input
    if parts and parts[0].lower() == root_name.lower():
        parts = parts[1:]
    return "/".join(parts)

def process_file(
    file_path: str,
    root_dir: str,
    out_dir: str,
    encoding: str = "utf-8",
) -> Tuple[int, int, Optional[str], str]:
    """
    Process one .txt file.

    Returns:
        lines_scanned: int
        logs_modified: int
        error_message_or_None: Optional[str]
        out_path_basename: str (final written filename)
    """
    rel_dir_posix = posix_rel_dir_excluding_root(root_dir, file_path)
    filename = os.path.basename(file_path)
    out_path = os.path.join(out_dir, filename)

    lines_scanned = 0
    logs_modified = 0

    try:
        with open(file_path, "r", encoding=encoding, errors="replace") as fin, \
             open(out_path, "w", encoding=encoding, errors="replace") as fout:

            for line in fin:
                lines_scanned += 1
                if TS_RE.match(line):
                    # Append subfolder path (root name excluded).
                    if rel_dir_posix:
                        # Handle both \n and \r\n gracefully
                        core = line.rstrip("\r\n")
                        nl = "\n" if line.endswith(("\n", "\r\n")) else ""
                        new_line = f"{core};{rel_dir_posix}{nl}"
                        if new_line != line:
                            logs_modified += 1
                        fout.write(new_line)
                    else:
                        # If any .txt are directly under root, just write as-is.
                        fout.write(line)
                else:
                    fout.write(line)

        return lines_scanned, logs_modified, None, os.path.basename(out_path)

    except Exception as e:
        return lines_scanned, logs_modified, f"{type(e).__name__}: {e}", ""

def iter_txt_files(root_dir: str, out_dir_abs: str, counters: Counters) -> List[str]:
    """
    Walk root_dir and collect .txt files. Exclude the output dir if it's inside root.
    Also skip known system/hidden folders and log them.
    """
    files = []
    for cur_dir, subdirs, filenames in os.walk(root_dir):
        # prune folders
        pruned = []
        for d in list(subdirs):
            full = os.path.join(cur_dir, d)
            # skip our output or known system/hidden folders
            name = os.path.basename(full)
            if (out_dir_abs and os.path.commonpath([os.path.abspath(full), out_dir_abs]) == out_dir_abs) \
               or name in SKIP_DIR_NAMES or name.startswith("."):
                pruned.append(full)
                subdirs.remove(d)
        if pruned:
            counters.add(total_dirs_skipped=len(pruned))
            for p in pruned:
                logging.info(f"Skipping folder: {p}")

        for fn in filenames:
            if fn.lower().endswith(".txt"):
                files.append(os.path.join(cur_dir, fn))
    return files

def print_progress(done: int, total: int, width: int = 42):
    """Simple in-place progress bar."""
    if total <= 0:
        return
    pct = done / total
    filled = int(width * pct)
    bar = "â–ˆ" * filled + "-" * (width - filled)
    sys.stdout.write(f"\rProcessing files: |{bar}| {done}/{total} ({pct:.0%})")
    sys.stdout.flush()
    if done == total:
        sys.stdout.write("\n")

def main():
    # Basic validation of top-of-file config
    if not INPUT_ROOT or INPUT_ROOT.strip() == "":
        print("ERROR: Please set INPUT_ROOT at the top of the script to your root folder path.", file=sys.stderr)
        sys.exit(2)

    try:
        workers = int(WORKERS)
    except Exception:
        print("ERROR: WORKERS must be an integer.", file=sys.stderr)
        sys.exit(2)
    if workers <= 0:
        print("ERROR: WORKERS must be >= 1.", file=sys.stderr)
        sys.exit(2)

    root_dir = os.path.abspath(INPUT_ROOT)
    if not os.path.isdir(root_dir):
        print(f"ERROR: Input path is not a directory: {root_dir}", file=sys.stderr)
        sys.exit(2)

    # Output + log are always in the folder you run the script from (CWD)
    run_dir = os.getcwd()
    out_dir = os.path.join(run_dir, OUTPUT_DIR_NAME)
    os.makedirs(out_dir, exist_ok=True)
    log_path = os.path.join(run_dir, LOG_FILENAME)

    setup_logging(log_path)
    logging.info("=== Run started ===")
    logging.info(f"Running from: {run_dir}")
    logging.info(f"Input root: {root_dir} (root name excluded from appended path)")
    logging.info(f"Output directory (flat): {out_dir}")
    logging.info(f"Log file: {log_path}")
    logging.info(f"Workers: {workers}")

    counters = Counters()

    out_dir_abs = os.path.abspath(out_dir) if os.path.commonpath([out_dir, root_dir]) == os.path.abspath(root_dir) else ""
    file_list = iter_txt_files(root_dir, out_dir_abs, counters)
    counters.add(total_txt_files_found=len(file_list))

    if counters.total_txt_files_found == 0:
        logging.warning("No .txt files found. Exiting.")
        print("No .txt files found in the input directory.", file=sys.stderr)
        logging.info("=== Run finished (no files) ===")
        return

    # Detect same-name files (duplicate basenames). We will KEEP the first occurrence, SKIP the rest.
    by_name: Dict[str, List[str]] = defaultdict(list)
    for fp in file_list:
        by_name[os.path.basename(fp)].append(fp)

    # Build selected list and skipped duplicates list
    selected_files: List[str] = []
    skipped_duplicates: List[str] = []
    kept_for_name: Dict[str, str] = {}

    for name, paths in by_name.items():
        # Keep first (by current enumeration order), skip the rest
        keep = paths[0]
        kept_for_name[name] = keep
        selected_files.append(keep)
        if len(paths) > 1:
            for dup in paths[1:]:
                skipped_duplicates.append(dup)

    counters.add(total_files_selected=len(selected_files),
                 total_files_skipped_duplicates=len(skipped_duplicates),
                 total_files_skipped=len(skipped_duplicates))

    if skipped_duplicates:
        logging.info(f"Duplicate base filenames detected: {len([n for n,p in by_name.items() if len(p)>1])} unique names.")
        for dup in skipped_duplicates:
            name = os.path.basename(dup)
            kept = kept_for_name[name]
            kept_rel = posix_rel_dir_excluding_root(root_dir, kept) or "(directly under root)"
            dup_rel  = posix_rel_dir_excluding_root(root_dir, dup) or "(directly under root)"
            logging.info(f"SKIP duplicate: '{name}' from '{dup_rel}'  (kept first occurrence from '{kept_rel}')")

    logging.info(f"Found .txt files: {counters.total_txt_files_found}")
    logging.info(f"Selected for processing (unique names): {counters.total_files_selected}")
    if counters.total_files_skipped_duplicates:
        logging.info(f"Files skipped due to same name: {counters.total_files_skipped_duplicates}")

    # Progress bar will reflect ALL files (processed + duplicates skipped)
    total_progress_units = counters.total_files_selected + counters.total_files_skipped_duplicates
    done_units = 0
    print_progress(done_units, total_progress_units)

    # Simulate progress for duplicates skipped (so bar advances)
    for _ in skipped_duplicates:
        done_units += 1
        print_progress(done_units, total_progress_units)

    # Process only the selected (unique-name) files in parallel
    def _work(fp: str):
        return process_file(
            file_path=fp,
            root_dir=root_dir,
            out_dir=out_dir,
        )

    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
        future_map = {ex.submit(_work, fp): fp for fp in selected_files}
        for fut in concurrent.futures.as_completed(future_map):
            src = future_map[fut]
            try:
                lines_scanned, logs_modified, err, out_basename = fut.result()
                if err:
                    counters.add(errors=1, total_files_skipped=1)
                    logging.error(f"Failed: {src} | {err}")
                else:
                    counters.add(
                        total_files_processed=1,
                        total_lines_scanned=lines_scanned,
                        total_logs_modified=logs_modified,
                    )
                    # Per-file debug is in file log only (not noisy in console)
                    logging.debug(
                        f"Processed: {src} -> {out_basename} | lines={lines_scanned}, modified={logs_modified}"
                    )
            except Exception as e:
                counters.add(errors=1, total_files_skipped=1)
                logging.exception(f"Unhandled error while processing {src}: {e}")
            finally:
                done_units += 1
                print_progress(done_units, total_progress_units)

    # Summary (ONLY in log file; console prints just a small line)
    logging.info("=== Summary ===")
    logging.info(f"Total .txt files found:            {counters.total_txt_files_found}")
    logging.info(f"Total unique basenames selected:   {counters.total_files_selected}")
    logging.info(f"Total files processed:             {counters.total_files_processed}")
    logging.info(f"Total files skipped (all reasons): {counters.total_files_skipped}")
    logging.info(f"  â””â”€ Skipped due to same name:     {counters.total_files_skipped_duplicates}")
    logging.info(f"Total directories skipped:         {counters.total_dirs_skipped}")
    logging.info(f"Total lines scanned:               {counters.total_lines_scanned}")
    logging.info(f"Total log lines modified:          {counters.total_logs_modified}")
    logging.info(f"Errors:                            {counters.errors}")
    logging.info("=== Run finished ===")

    # Keep console minimal per your request
    print("\nDone. See detailed results in:", log_path)

if __name__ == "__main__":
    main()
